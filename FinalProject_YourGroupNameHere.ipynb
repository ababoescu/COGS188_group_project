{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Chess Game\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Ana Maria Baboescu\n",
    "- Fatima Enriquez\n",
    "- Eric Lin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "Our goal with this project is to create an artificial intelligence (AI) algorithm that can properly play a game of chess. The data that we used for this project was solely exploration of potential future states within the game itself. We used pyChess in order to create this game, and created and updated values based on potential future states according to a Monte Carlo simulation. The performance of the model is measured by the cumulative rewards that were achieved during a game of chess against an opponent who uses completely random moves. High rewards imply good performance, while low rewards imply poorer performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "There has been ample research regarding the use of AI to solve problems. Due to the increase in popularity of online games such as chess, sudoku, and solitaire, more individuals are inclined to play them virtually. Having this virtual access was key, especially during the pandemic <a name=\"cite_2\"></a>[<sup>[2]</sup>](#cite_2). However, the concern for many is how to detect cheating. The European Online Championship details that it is paramount to keep the integrity of the game to the point where more than eight participants, in 2020, were deemed disqualified for utilizing aid <a name=\"cite_3\"></a>[<sup>[3]</sup>](#cite_3). In this manner, there is a desire for softwares to be able to detect cheating in an online game <a name=\"cite_1\"></a>[<sup>[1]</sup>](#cite_1). The use of AI in online chess matches makes it easier for the person cheating to win the game. While it is fascinating to imagine how a program can win such a strategic game, ethically, it is paramount that as a society we use AI as a learning tool.\n",
    "\n",
    "Nonetheless, the desire to solve problems and expand the realm of artificial intelligence has been around for decades. In one of the most infamous games of chess played by Garry Kasparov and Deep Blue, the machine won by one point <a name=\"cite_1\"></a>[<sup>[1]</sup>](#cite_1). Deep Blue is a software created by International Business Machines Corporation (IBM), which consisted of 32 processors and produced “high-speed computations in parallel”. Deep Blue was thus able to evaluate about 200 million chess positions per second and held a “processing speed of 11.38 billion floating-point operations per second” <a name=\"cite_4\"></a>[<sup>[4]</sup>](#cite_4).This breakthrough to be able to have a machine detect and think like a human is evolutionary to the point where the legendary match, where Deep Blue won was due to the computational build of the machine. This is because the machine has such a high processing speed and evaluation power <a name=\"cite_1\"></a>[<sup>[1]</sup>](#cite_1).\n",
    "\n",
    "Aside from one of the most historical events in the history of AI, about half of a century later, the artificial intelligence system Hydra came along. In essence Hydra was created and played against chess Champion Vladimir Kramnik in 2005 <a name=\"cite_8\"></a>[<sup>[8]</sup>](#cite_8) . Their system sparked the continuation of developing tools to help others build their algorithms. Essentially, the Stockfish engine is available for those who desire to analyze and compute pieces and positions of the chess game  <a name=\"cite_10\"></a>[<sup>[10]</sup>](#cite_10). With the development of Stockfish to the community of chess players, this chess engine won ample amounts of matches. Such matches include the World Computer Chess and Top Chess Engine Championships <a name=\"cite_8\"></a>[<sup>[8]</sup>](#cite_8) . The manner in which this chess engine functions is by implementing the minimax, pruning, and extensions implementations. Essentially, what occurs is that the system branches out to analyze the different child notes of possible moves and subsequently decides what is the most optimal move <a name=\"cite_10\"></a>[<sup>[10]</sup>](#cite_10). Yet, even with this major development, the Stockfish algorithm was defeated in 2017 by AI Alpha Zero. Creators DeepMind won almost 30% of the games against Stockfish with a draw of roughly 70%. This in itself is a major accomplishment as this AI system could then over take the human chess champions in an efficient manner. The major difference between Hydra and AlphaZero was the implementation of their algorithms. As Mazurek recalled in the Medium article, AlphaZero utilized reinforcement learning within its structure. In turn, this allowed AlphaZero to significantly outperform Hyra <a name=\"cite_8\"></a>[<sup>[8]</sup>](#cite_8). As detailed in the textbook, there is a consequent reward allocated after winning or losing the game. In this manner, the agent will learn to try and reach the positive reward at the end of its task. In the case of the chess game, it would be to reach a checkmate. As detailed in the text, there is an “unknown Markov decision process” at play where the agent selects states that maximize their likelihood of getting their desired outcome. Subsequently, the agent in the game learns a “Q-function” where they calculate how much their set of state and action costs (Russell and Norvig, 2009, pg 831). All in all since the development and advancements of technology today, artificial intelligence is significantly improving in all aspects such as computational efficiency and being able to beat the previous best AI system. \n",
    "\n",
    "In a similar manner, there are other games that have been tackled that have been played by AI systems. One of these games is one called Backgammon, which ties in the concepts of “reinforcement learning and neural networks” in order to gain a greater understanding of what to select next. The manner in which the system improves its performance is by repeatedly playing against itself. Another game that has been utilized as an inspiration for bringing AI into the field is GO. Within the game of Go, there is a board that is 19 units by 19 units. This means that there are 19 columns of cells and 19 columns or rows. Within the game, the AI system does not prune the game, instead, it utilizes the Monte Carlo algorithm to solve its problems (Russell and Norvig, 2009, pg 185-187). As one can note, there are many other games that could involve AI; however, coming in line with the purpose of this project the aforementioned games are in line with the complexity with the game of chess. In this manner, what our group attempted to solve was to implement an algorithm that would choose the optimal move at each state. This in itself is a complex task to accomplish because of the diverse set of rules in a game such as Chess and Go. There are many different loopholes in which the agent needs to learn how to play. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Clearly describe the problem that you are solving. Avoid ambiguous words. The problem described should be well defined and should have at least one ML-relevant potential solution. Additionally, describe the problem thoroughly such that it is clear that the problem is quantifiable (the problem can be expressed in mathematical or logical terms), measurable (the problem can be measured by some metric and clearly observed), and replicable (the problem can be reproduced and occurs more than once).\n",
    "\n",
    "\n",
    "One of the widely known games, Chess, is a game typically associated with strategy and skill with multiple different styles of playing/executing. Thus, the success of the player is determined by the manner in which both players move. This can frequently be quite difficult for beginners, because without AI, one would have to seek different skill-leveled individuals to build one's mastery of the game. However, with AI, each time the player plays the system, it would be different for the player due to the reinforcement learning of past games it has played including its experience with the current player."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "For our data we did not implement any datasets due to the lack of not having the need for the. This is due to the fact that we implemented the Monte Carlo algorithm for our AI agent to follow. For our allocated data, we mainly created our own by analyzing how our Monte Carlo algorithm was learning. The manner in which the AI agent learned using the aforementioned algorithm was from the game simulation and observing the data through graphing the overall rewards per move that was gained throughout the chess game. \n",
    "\n",
    "**In regards to needing a dataset, we have consulted a TA to see if we needed one, we were informed that a dataset was not required if we did not need one. Thus, due to the aspects and details of our project, we formulated tangible visual representations of what the AI agent went through.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "For our project, we propose to create an algorithm using Monte Carlo and reinforcement learning. While chess in itself is not a stochastic game, rather a deterministic game, due to the lack of random-ness and unpredictability, we can still apply the Monte Carlo algorithm to our game (Russell and Norvig, 2009, pg 177). Essentially, we decided to utilize the Monte Carlo algorithm over others such as the Minimax because by utilizing the latter, we were able to compute visualizations. In this manner, we would see how the agent learned to play and thus provide data on how it learned. In a human chess game, both players typically attempt to play optimally, in order to win. One would win by trying to take as many pieces from the opponent as possible; thus clearing the board, and attempting to checkmate. This makes the minimax algorithm, one of the ideal algorithms to use for reaching the aforementioned goal.However, in the case that we did utilize the minimax algorithm, this could essentially compute the optimal move and not provide tangible data for us to collect. This is because the minimax algorithm determines the optimal move at a specified state by “performing a complete depth-first” search of the possible moves(Russell and Norvig, 2009,pg 165).\n",
    "Subsequently, according to Russel and Norvig, reinforcement learning is defined as an active process where an agent in a specified environment learns through allocated consequences (2009, pg 695). In this manner, reinforcement learning is used by the AI to help utilize what the algorithm learned from previous games in order to improve its performance in the next game and possibly recognize patterns. This is paramount because in the context of being able to train a model to navigate through an environment and win, this can get quite complex very quickly. By being able to recognize patterns of players, the AI could have more experience as well as adapt to different styles of playing.\n",
    "\n",
    "We would be utilizing numpy to calculate mathematical calculations. Also, the library, python-chess library provides all the move generations, validations, and board/pieces of the game itself. Within the algorithm itself, we adapted the backbone from Ishaan Gupta work on the *Monte Carlo Tree Search Application on Chess*. The Node class allowed us to create a node object that will navigate itself through the game. Subsequently,the ucb1 function takes in the current node. Here, we utilized the Upper Confidence Bound algorithm which essentially selects the move with the highest upper bound, where this would provide either the move with the highest value or reward <a name=\"cite_14\"></a>[<sup>[14]</sup>](#cite_14).  The following function, evaluate_board, evaluates each of the game pieces and returns a score. There is a specified dictionary which pairs each of the game pieces (king, queen, pawn, etc) to assigned numerical values. We also implemented the rollout function that details the resulting score at the end of each game. It does this by evaluating the different possible moves, derived from the expand function, until the end of the game <a name=\"cite_5\"></a>[<sup>[5]</sup>](#cite_5).  In turn, the referenced expand function which runs through, recursively through the game. Thus, once the “current leaf node” is reached then this is when the call to the child nodes ends <a name=\"cite_5\"></a>[<sup>[5]</sup>](#cite_5).  The rollback function updates the nodes in the game tree with the specified results of the aforementioned function. Starting from the end of the nodes, this function iterates and updates the rewards to each of the nodes. One of the main functions, mcts_pred in sum, predicts the best move to execute in the game. After iterating through each of the allocated moves in the game of class, the function utilizes the Upper Confidence Bound algorithm to choose the best one. In order to implement some randomness, essentially capturing the essence of Monte Carlo, we used a random legal moves function to randomize the possible legal moves. With the rest of the program, we initialized all of the possible moves and chose the allocated move  for both of the colors (black and white).\n",
    "\n",
    "With all of these functions in mind to make this program possible, we essentially solved what we detailed in the problem statement which was to utilize reinforcement learning as a means of training an AI agent to efficiently play chess. The model which our AI agent is being compared to is essentially whether it wins or loses the game. Subsequently, we want to ensure that the AI agent following the Monte Carlo algorithm is increasing its rewards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "The evaluation metric that we utilized to visualize the performance of our model is to create a graph that highlights the change in reward over the processed moves. As aforementioned in the proposed solution method, we implemented the Monte Carlo algorithm to be able to visualize how the AI agent learned to play. By doing so, we were able to visualize and quantify the performance of the AI agent. The manner in which we quantified the end performance was by computing the result at the end of the game, thus the agent either won, lost, or tied in the game. The agent, who utilized the Monte Carlo Algorithm, played against another agent which generated only random valid moves. \n",
    "\n",
    "The manner in which the evaluation metrics were derived was by taking the reward of the AI agent at each of the states. When the AI agent follows a possible future win in the chessboard, it gets rewarded 10 points. However, when the AI agent follows a possible future loss, then the agent gets penalized 10 points. However, one of the patterns that we noted was how the AI agent kept getting into stalemates with the other agent. This was partly due to the fact that the AI agent was navigating against each of the moves that the random generating agent was making at the current moment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import chess.pgn\n",
    "import chess.engine\n",
    "import chess.svg\n",
    "import random\n",
    "from math import log, sqrt, e, inf\n",
    "#from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "# for the gif\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "depth = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsection 1 - Implementing the Node Class & Upper Confidence Bound\n",
    "\n",
    "Before implementing the Monte Carlo algorithm, we first created a Node class since we were specifically focused on implementing a Monte Carlo Tree Search (MCTS). The reason for us implementing a Monte Carlo Tree Search is due to its main advantage of being able to operate effectively without prior knowledge over certain domains (exempting the general rules and termination states of the game) through discovering its own moves and learning through random plays. This leads to it providing the best probabilistic move given the current state of the game. Because of this it, is widely used in game theory like chess.\n",
    "\n",
    "\n",
    "On top of creating the Node class for this algorithm, we have also implement an Upper Confidence Bound (UCB) which, according to a medium article by Ishaan Gupta on “Monte Carlo Tree Search Application on Chess”, helps in deciding which node to evaluate based on maximizing the probability of winning from the given state <a name=\"cite_5\"></a>[<sup>[5]</sup>](#cite_5). UCB follows the general equation:\n",
    "\n",
    "$$A_t=argmax_a(Qt(a) + c\\sqrt{\\frac{ln(t)}{N_t(a)}})$$\n",
    "\n",
    "which consists of two factors, exploitation and exploration.\n",
    "\n",
    "The exploitation factor, that is represented by $Qt(a)$, measures how successful the action (a) is whenever it is used. To put it simply, the higher the success rate of the exploitation the higher the UCB value is. The exploration factor, $c\\sqrt{\\frac{ln(t)}{N_t(a)}}$ explores the possible actions to take. The UCB factor is calculated in the ucb1 function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self):\n",
    "        self.state = chess.Board() # current position of board\n",
    "        self.children = set() # set of all possible states from legal action from current node\n",
    "        self.parent = None # parent node of current node\n",
    "        self.N = 0 # number of times parent node has been visited\n",
    "        self.n = 0 # number of times current node has been visited\n",
    "        self.v = 0 # exploitation factor of current node\n",
    "        self.ucb = 0 # Upper confidence bound\n",
    "    def __lt__(self, other):\n",
    "        return self.ucb < other.ucb\n",
    "\n",
    "def ucb1(curr_node):\n",
    "    ans = curr_node.v + 2 * (sqrt(log(curr_node.N + e + (10 ** -6))/(curr_node.n + (10 ** -10))))\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsection 2\n",
    "\n",
    "For the MCTS algorithm we implemented several helper functions that would be used when implementing the main algorithm. These functions are:\n",
    "- **Evaluate_board:** This function evaluates the pieces on the board and calculator values to help determine which pieces are more ideal to take.\n",
    "- **Rollout:** This function generates random moves from the current node until termination  and returns either the reward or punishment with the current node.\n",
    "- **Expand:** This function keeps calling the child node for a certain turn and will return the maximum priority till the current leaf node has been reached.\n",
    "- **Rollback:** After the final node and reward is read, this function traverses back to the root and in turn updates the UCB value for each node of the path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_board(board):\n",
    "    piece_values = {\n",
    "        chess.PAWN: 1,\n",
    "        chess.KNIGHT: 3,\n",
    "        chess.BISHOP: 3,\n",
    "        chess.ROOK: 5,\n",
    "        chess.QUEEN: 10,\n",
    "        chess.KING: 0\n",
    "    }\n",
    "    value = 0\n",
    "    for piece_type in piece_values:\n",
    "        value += len(board.pieces(piece_type, chess.WHITE)) * piece_values[piece_type]\n",
    "        value -= len(board.pieces(piece_type, chess.BLACK)) * piece_values[piece_type]\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(curr_node, reward):\n",
    "    board = curr_node.state \n",
    "    depth = 0\n",
    "    captured_piece = None\n",
    "    while not board.is_game_over():\n",
    "        legal_moves = list(board.legal_moves)\n",
    "        move_weights = []\n",
    "        for move in legal_moves:\n",
    "            board.push(move)\n",
    "            move_weights.append(evaluate_board(board))\n",
    "            board.pop()\n",
    "        total_weight = sum(move_weights)\n",
    "        if total_weight == 0:\n",
    "            move = random.choice(legal_moves)\n",
    "        else:\n",
    "            probabilities = [weight / total_weight for weight in move_weights]\n",
    "            move = random.choices(legal_moves, probabilities)[0]\n",
    "        if board.is_capture(move):\n",
    "            if board.is_en_passant(move):\n",
    "                captured_piece = chess.PAWN\n",
    "            else:\n",
    "                captured_piece = board.piece_at(move.to_square).piece_type\n",
    "        if captured_piece is not None and depth == 0:\n",
    "            if captured_piece == chess.PAWN:\n",
    "                reward += 1\n",
    "            elif captured_piece == chess.KNIGHT:\n",
    "                reward += 3\n",
    "            elif captured_piece == chess.BISHOP:\n",
    "                reward += 3\n",
    "            elif captured_piece == chess.ROOK:\n",
    "                reward += 5\n",
    "            elif captured_piece == chess.QUEEN:\n",
    "                reward += 9\n",
    "        board.push(move)\n",
    "        depth += 1\n",
    "\n",
    "    if board.is_game_over():\n",
    "        result = board.result()\n",
    "        if result == '1-0':\n",
    "            return reward + 10\n",
    "        elif result == '0-1':\n",
    "            return reward - 10\n",
    "        else:\n",
    "            return reward\n",
    "    return rollout(curr_node, reward)\n",
    "    #return evaluate_board(board) / 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(curr_node, white):\n",
    "    if len(curr_node.children) == 0:\n",
    "        return curr_node\n",
    "    max_ucb = -inf\n",
    "    if white:\n",
    "        sel_child = None\n",
    "        for i in curr_node.children:\n",
    "            tmp = ucb1(i)\n",
    "            if tmp > max_ucb:\n",
    "                max_ucb = tmp\n",
    "                sel_child = i\n",
    "        return expand(sel_child, 0)\n",
    "    else:\n",
    "        sel_child = None\n",
    "        min_ucb = inf\n",
    "        for i in curr_node.children:\n",
    "            tmp = ucb1(i)\n",
    "            if tmp < min_ucb:\n",
    "                min_ucb = tmp\n",
    "                sel_child = i\n",
    "        return expand(sel_child, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = [] # list that stores rewards\n",
    "\n",
    "def rollback(curr_node, reward):\n",
    "    curr_node.n += 1\n",
    "    curr_node.v += reward\n",
    "    while curr_node.parent is not None:\n",
    "        curr_node.N += 1\n",
    "        curr_node = curr_node.parent\n",
    "    rewards.append(reward) # Append the reward to the list\n",
    "    return curr_node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsection 3 - Implementing the Monte Carlo & Random Move Generator\n",
    "\n",
    "In this section we implement the Monte Carlo Tree Search algorithm by calling all the helper functions together and calculate the rewards. We also implemented a random legal move generator function that randomly generates possible legal moves for an opponent computer to play.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcts_pred(curr_node, over, white, iterations=5000): #updated iterations from 10 to 100\n",
    "    if over:\n",
    "        return -1\n",
    "    all_moves = [curr_node.state.san(i) for i in list(curr_node.state.legal_moves)]\n",
    "    map_state_move = dict()\n",
    "\n",
    "    for i in all_moves:\n",
    "        tmp_state = chess.Board(curr_node.state.fen())\n",
    "        tmp_state.push_san(i)\n",
    "        child = Node()\n",
    "        child.state = tmp_state\n",
    "        child.parent = curr_node\n",
    "        curr_node.children.add(child)\n",
    "        map_state_move[child] = i\n",
    "\n",
    "    while iterations > 0:\n",
    "        if white:\n",
    "            max_ucb = -inf\n",
    "            sel_child = None\n",
    "            for i in curr_node.children:\n",
    "                tmp = ucb1(i)\n",
    "                if tmp > max_ucb:\n",
    "                    max_ucb = tmp\n",
    "                    sel_child = i\n",
    "\n",
    "            ex_child = expand(sel_child, 0)\n",
    "            reward = rollout(ex_child, 0)\n",
    "            curr_node = rollback(ex_child, reward)\n",
    "            iterations -= 1\n",
    "        else:\n",
    "            min_ucb = inf\n",
    "            sel_child = None\n",
    "            for i in curr_node.children:\n",
    "                tmp = ucb1(i)\n",
    "                if tmp < min_ucb:\n",
    "                    min_ucb = tmp\n",
    "                    sel_child = i\n",
    "\n",
    "            ex_child = expand(sel_child, 1)\n",
    "            reward = rollout(ex_child)\n",
    "            curr_node = rollback(ex_child, reward)\n",
    "            iterations -= 1\n",
    "        \n",
    "        if white:\n",
    "            max_ucb = -inf\n",
    "            selected_move = ''\n",
    "            for i in curr_node.children:\n",
    "                tmp = ucb1(i)\n",
    "                if tmp > max_ucb:\n",
    "                    max_ucb = tmp\n",
    "                    selected_move = map_state_move[i]\n",
    "            return selected_move\n",
    "        else:\n",
    "            min_ucb = inf\n",
    "            selected_move = ''\n",
    "            for i in curr_node.children:\n",
    "                tmp = ucb1(i)\n",
    "                if tmp < min_ucb:\n",
    "                    min_ucb = tmp\n",
    "                    selected_move = map_state_move[i]\n",
    "            return selected_move\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomLegalMove(curr_node):\n",
    "    legal_moves = list(curr_node.state.legal_moves)\n",
    "    rand = random.randrange(len(legal_moves))\n",
    "    return board.san(legal_moves[rand])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsection 4 - Main Function\n",
    "\n",
    "The main function calls upon the board and implements the game. It helps keep track of the status of the game and the different turns by switching between players. At the end it provides the results of the game along with the layout of the final board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main Function\n",
    "board = chess.Board()\n",
    "\n",
    "white = 1\n",
    "moves = 0\n",
    "pgn = []\n",
    "game = chess.pgn.Game()\n",
    "evalutations = []\n",
    "sm = 0\n",
    "cnt = 0\n",
    "\n",
    "# store each board state as an image\n",
    "board_states = []\n",
    "#root = Node()\n",
    "while not board.is_game_over():\n",
    "    if white == 1:\n",
    "        \n",
    "\n",
    "        root = Node()\n",
    "        root.state = board\n",
    "        result = mcts_pred(root, board.is_game_over(), white, iterations=500) # added in iterations = 100\n",
    "\n",
    "        board.push_san(result)\n",
    "\n",
    "        pgn.append(result)\n",
    "        white ^= 1 # allows to switch between 2 different states black and white\n",
    "\n",
    "        # Save the current board state\n",
    "        board_states.append(board.copy())\n",
    "        moves += 1\n",
    "    else:\n",
    "        \n",
    "        root = Node()\n",
    "        root.state = board\n",
    "        result = getRandomLegalMove(root)\n",
    "        board.push_san(result)\n",
    "        pgn.append(result)\n",
    "        white ^= 1 \n",
    "        board_states.append(board.copy())\n",
    "        moves += 1\n",
    "\n",
    "board_states.append(board.copy())\n",
    "\n",
    "print(board)\n",
    "print(' '.join(pgn))\n",
    "print()\n",
    "print(board.result())\n",
    "game.headers['Result'] = board.result()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsection 5 - Gif and Graphs\n",
    "\n",
    "After running all the Monte Carlo algorithms, we graphed the rewards for each of the move numbers to observe how the algorithm is learning throughout the whole game. As described in the previous subsections, we assigned different values to the different pieces to place higher importance on certain pieces versus others allowing the algorithm to prioritize the pieces with higher scores. This has also led to the overall rewards on the graph with each iteration of moves growing throughout the game, as can be seen from the graph below. Along with the graph we also provided a gif of the chess game being played for you to observe. In this game, **white** is using the Monte Carlo method while **black** is using a random move generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load unicode \n",
    "pieces_unicode = {\n",
    "    \"P\": \"♙\", \"R\": \"♖\", \"N\": \"♘\", \"B\": \"♗\", \"Q\": \"♕\", \"K\": \"♔\",\n",
    "    \"p\": \"♟\", \"r\": \"♜\", \"n\": \"♞\", \"b\": \"♝\", \"q\": \"♛\", \"k\": \"♚\" \n",
    "}\n",
    "\n",
    "# Create animation using FuncAnimation (from L6)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "def plot_board(board, ax):\n",
    "    ax.clear()\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.imshow([[1, 0] * 4, [0, 1] * 4] * 4, cmap='gray', alpha=0.3) # gray and white\n",
    "    for square in chess.SQUARES:\n",
    "        piece = board.piece_at(square)\n",
    "        if piece:\n",
    "            piece_unicode = pieces_unicode[piece.symbol()]\n",
    "            ax.text(square % 8, 7 - square // 8, piece_unicode, fontsize=36, ha='center', va='center')\n",
    "\n",
    "def update(frame):\n",
    "    plot_board(board_states[frame], ax)\n",
    "\n",
    "animation = FuncAnimation(fig, update, frames=len(board_states), repeat=False)\n",
    "animation.save('chess_game_2.gif', writer='pillow', fps=3)\n",
    "plt.close(fig)\n",
    "\n",
    "cumulative_rewards = []\n",
    "sum = 0\n",
    "for val in rewards:\n",
    "    sum += val\n",
    "    cumulative_rewards.append(sum)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cumulative_rewards, marker='o')\n",
    "plt.title('Updating Rewards Over the Game')\n",
    "plt.xlabel('Move Number')\n",
    "plt.ylabel('Reward')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![winning chess game reward png](winning_reward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![chess_game gif](winning_game.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "### Interpreting the result\n",
    "\n",
    "In the graph above, which led to a win for **white**, the Monte Carlo algorithm, the reward steadily increases over the course of the game. As the algorithm makes moves that are the most likely to lead **white** to a victory, the reward increases. Meanwhile, in graphs of games that lead **white** to a loss, the reward tends to steadily go downwards. The reward is correlated to actions that the algorithm has sampled and detected a checkmate, as well as capturing the opponent’s pieces which results in a material advantage. \n",
    "\n",
    "However, this isn’t always the case. While the reward graphs for wins generally tend to trend upwards and the reward graphs for losses tend to trend downwards, on rare occasions, a positive reward graph can result in a loss and a negative reward graph can result in a win. \n",
    "\n",
    "![negative reward win png](negative_reward_win.png)\n",
    "\n",
    "The above graph in which **white** won the game shows one such example; despite winning the game, the reward for the graph is in the negatives. The fact that there was a checkmate hidden beyond all the negative rewards implies that the algorithm did not see this checkmate until likely around move 178. This shows that, due to the randomness of Monte Carlo, in addition to the randomness of the random opponent, the algorithm still has many blind spots that it cannot see until it is right next to it.\n",
    "\n",
    "But by far, the most frequent outcome of the Monte Carlo vs Random Algorithm is a tie. The graphs for ties often look very similar to the graphs of wins or losses, with the graphs tending to have a positive or negative trend. Despite this, they appear to get stuck for a while until the game results in a stalemate. \n",
    "\n",
    "![draw reward png](draw_reward.png)\n",
    "\n",
    "This graph for a stalemate looks rather similar to the winning graph provided earlier. This is likely due to the fact that the algorithm detects a checkmate in the extremely near future, but doesn’t know how to adapt to the unpredictable actions of the random action opponent. As such, despite having an advantage and being really close to a checkmate, it dances around the checkmate until one side gets stuck in a position in which they cannot make a move, causing a tie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "\n",
    "One limitation with the work is simply the limitations of computers and the nature of the game of chess. It is estimated that there are approximately $10^{100}$ possible positions for a chess board to be in, so it is impossible to update a Q-table in this regard. We got around this by using a Node() class and updating values inside each node during the simulation stages. While this solved the problem, there are still issues such as different steps to get to the same state ending up as two different nodes and having two different values. \n",
    "\n",
    "Another limitation is that we had the chess AI play against a random algorithm in order to train it. Random algorithms do not act like humans and thus do not do certain things that would be beneficial, such as capturing a piece when given the opportunity. As such, the model may not be best equipped for playing against humans. \n",
    "   \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "One potential concern regarding ethics that may come with this project would be the ability to use the AI to cheat in real online chess matches against unsuspecting players. For example, a cheater may be able to hook up the AI directly to the chess program or they may be able to use it to simulate future moves, gaining an unfair advantage. I think a way to address this may be to warn players that many multiplayer chess sites have anti-cheating detection, discouraging the use of this algorithm in cheating.\n",
    "\n",
    "Another concern is that this algorithm is fairly resource-intensive. Scaling this algorithm to be significantly more efficient and effective enough to beat professional chess players would likely require very large amounts of processing power and time in order to trai, which would be bad for the environment.\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "All in all, by utilizing the Monte Carlo algorithm we were successful in being able to train the AI agent to successfully navigate/learn the game of chess against an opponent computing random moves. Essentially, our results support our problem statement because, as aforementioned, despite the lack of the AI agent losing the game, there were still rewards and learning progress. Thus, this in itself would fit in the context of other work being created in the gaming industry. As detailed in the background section, there are ample Chess Championship matches occurring all around the world; hence, having an AI agent to practice against would be key to build one's skill-level. This is because one could train the AI model to exceed the individual's expertise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"cite_1\"></a>1.[^](#cite_1): Duca Iliescu, D. M. (10 Dec 2020) The Impact of Artifical Intelligence on the Chess World. *National Library of Medicine*. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7759436/<br>\n",
    "<a name=\"cite_2\"></a>2.[^](#cite_2): Waldstein, D. (8 May 2020) Chess Thrives Online Despite Pandemic. *The New York Times*. https://www.nytimes.com/2020/05/08/sports/coronavirus-chess-online-tournament.html <br>\n",
    "<a name=\"cite_3\"></a>3.[^](#cite_3): Schormann, C. (21 May 2020) Can online chess overcome cheating? *ChessTech* https://www.chesstech.org/2020/can-online-chess-overcome-cheating/ <br>\n",
    "<a name=\"cite_4\"></a>4.[^](#cite_4): Deep Blue. IBM. (n.d.). *IBM* https://www.ibm.com/history/deep-blue <br>\n",
    "<a name=\"cite_5\"></a>5.[^](#cite_5): Gupta, I. (2020a, November 13). Monte Carlo Tree Search application on chess. *Medium*. https://medium.com/@ishaan.gupta0401/monte-carlo-tree-search-application-on-chess-5573fc0efb75  <br>\n",
    "<a name=\"cite_6\"></a>6.[^](#cite_6):  F, J. (2024, April 6). Please help creating an chess board image based on description. *OpenAI Developer Forum*. https://community.openai.com/t/please-help-creating-an-chess-board-image-based-on-description/645399/7 <br>\n",
    "<a name=\"cite_7\"></a>7.[^](#cite_7): Roy, R. (2023, May 23). ML: Monte Carlo Tree Search (MCTS). *GeeksforGeeks*. https://www.geeksforgeeks.org/ml-monte-carlo-tree-search-mcts/ <br>\n",
    "<a name=\"cite_8\"></a>8.[^](#cite_8): Mazurek, D. (2022, December 19). AI Journey — from chess to software development!. *Medium*.https://medium.com/@damian.s.mazurek/ai-path-from-chess-to-software-development-b99425ec12d1 <br>\n",
    "<a name=\"cite_9\"></a>9.[^](#cite_9): Byrne, R. (2005, July 10). It’s man vs. machine again, and man comes out limping. *The New York Times*. https://www.nytimes.com/2005/07/10/crosswords/chess/its-man-vs-machine-again-and-man-comes-out-limping.html <br>\n",
    "<a name=\"cite_10\"></a>10.[^](#cite_10): Stockfish. (n.d.). https://disservin.github.io/stockfish-docs/stockfish-wiki/Home.html <br>\n",
    "<a name=\"cite_11\"></a>11.[^](#cite_11): What is depth?. *Stockfish*. (2024, January 21). https://disservin.github.io/stockfish-docs/stockfish-wiki/Stockfish-FAQ.html#minimax <br>\n",
    "<a name=\"cite_12\"></a>12.[^](#cite_12): Russell, S., & Norvig, P. (2009). Artificial Intelligence: A Modern Approach. *Pearson*. <br>\n",
    "<a name=\"cite_13\"></a>13.[^](#cite_13): Ish2K. (2020, November 13). Chess-bot-ai-algorithms/git_chess/monte_carlo_implementation.py at main · Ish2K/Chess-Bot-ai-algorithms. GitHub. https://github.com/Ish2K/Chess-Bot-AI-Algorithms/blob/main/Git_chess/monte_carlo_implementation.py <br>\n",
    "<a name=\"cite_14\"></a>14.[^](#cite_14): GeeksforGeeks. (2020, February 19). Upper Confidence Bound Algorithm in Reinforcement Learning.* GeeksforGeeks*. https://www.geeksforgeeks.org/upper-confidence-bound-algorithm-in-reinforcement-learning/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contributions\n",
    "**Fatima:** Background, Proposed Solution, Research, Evaluation Metrics\n",
    "\n",
    "**Ana:** Results, Data, Proposed Solution, Research, Visualizations\n",
    "\n",
    "**Eric:** Research, Discussion, Ethics and Privacy, Visualizations\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
